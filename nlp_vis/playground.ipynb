{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.26.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE=\"/Users/jiayixian/cache\" # /Users/jiayixian/.cache/huggingface/hub\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('max.colwidth', 0)\n",
    "\n",
    "data_path = \"\"\n",
    "model_path = \"/Users/jiayixian/cache/models--LightChen2333--joint-bert-slu-atis\"\n",
    "col_mapping = { # fix_name: col name\n",
    "    'label': 'label',\n",
    "    'text': 'text'\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df_data = pd.read_csv(data_path)\\nlabel_list = list(df_data[col_mapping['label']].unique())\\nlabel_list.sort()\\nnum_labels = len(label_list)\\nprint(num_labels, label_list)\\nlabel2id = { v: i for i, v in enumerate(label_list)}\\nid2label = { i: v for i, v in enumerate(label_list)}\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"df_data = pd.read_csv(data_path)\n",
    "label_list = list(df_data[col_mapping['label']].unique())\n",
    "label_list.sort()\n",
    "num_labels = len(label_list)\n",
    "print(num_labels, label_list)\n",
    "label2id = { v: i for i, v in enumerate(label_list)}\n",
    "id2label = { i: v for i, v in enumerate(label_list)}\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/jiayixian/cache/models--LightChen2333--joint-bert-slu-atis were not used when initializing BertForSequenceClassification: ['model.encoder._AutoEncoder__encoder.encoder.encoder.layer.11.attention.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.9.attention.self.key.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.3.output.LayerNorm.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.6.attention.self.query.weight', 'model.encoder._AutoEncoder__encoder.encoder.embeddings.position_embeddings.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.3.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.7.attention.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.0.attention.self.key.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.11.attention.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.10.intermediate.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.8.intermediate.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.6.attention.self.key.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.9.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.3.attention.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.3.intermediate.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.5.attention.self.value.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.6.intermediate.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.4.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.3.attention.self.key.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.5.attention.self.value.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.7.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.11.intermediate.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.0.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.3.intermediate.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.8.attention.self.value.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.8.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.10.output.LayerNorm.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.11.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.2.intermediate.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.8.attention.self.query.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.4.attention.self.query.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.5.attention.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.6.attention.self.value.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.9.attention.self.key.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.6.attention.self.value.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.10.attention.self.query.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.9.intermediate.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.11.intermediate.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.6.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.pooler.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.embeddings.LayerNorm.bias', 'model.encoder._AutoEncoder__encoder.encoder.embeddings.position_ids', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.11.attention.self.key.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.3.attention.self.query.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.3.attention.self.value.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.1.intermediate.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.2.attention.self.value.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.2.output.LayerNorm.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.2.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.4.attention.self.value.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.11.attention.self.value.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.5.attention.self.query.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.4.output.LayerNorm.bias', 'model.decoder.intent_classifier.intent_classifier.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.10.intermediate.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.10.attention.self.query.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.10.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.11.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.4.attention.self.query.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.7.attention.self.query.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.3.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.11.attention.self.query.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.5.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.1.output.LayerNorm.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'model.decoder.slot_classifier.slot_classifier.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.2.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.9.attention.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.3.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.embeddings.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.9.attention.self.value.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.2.attention.self.value.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.5.attention.self.key.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.1.intermediate.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.1.attention.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.4.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.1.attention.self.value.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.8.output.LayerNorm.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.11.attention.self.key.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.7.attention.self.value.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.0.output.dense.weight', 'model.decoder.slot_classifier.slot_classifier.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.7.attention.self.key.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.4.attention.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.5.attention.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.4.attention.self.key.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.6.intermediate.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.5.output.LayerNorm.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.1.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.1.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.4.intermediate.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.3.attention.self.value.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.2.attention.self.key.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.2.attention.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.8.attention.self.key.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.7.attention.self.key.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.6.attention.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.6.output.LayerNorm.bias', 'model.decoder.intent_classifier.intent_classifier.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.8.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.6.attention.self.key.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.9.attention.self.query.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.11.attention.self.query.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.0.attention.self.value.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.10.attention.self.value.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.2.attention.self.key.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.1.attention.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.0.attention.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.4.attention.self.value.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.5.attention.self.key.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.4.attention.self.key.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.3.attention.self.key.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.3.attention.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.11.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.2.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.6.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.0.output.LayerNorm.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.8.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.7.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.10.attention.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.0.attention.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.2.attention.self.query.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.7.attention.self.value.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.1.attention.self.value.bias', 'model.encoder._AutoEncoder__encoder.encoder.embeddings.token_type_embeddings.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.9.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.1.attention.self.query.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.5.intermediate.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.7.attention.self.query.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.1.attention.self.key.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.5.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.8.attention.self.query.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.10.attention.self.key.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.0.intermediate.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.11.attention.self.value.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.11.output.LayerNorm.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.6.attention.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.10.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.0.attention.self.value.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.7.attention.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.9.attention.self.value.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.6.attention.self.query.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.4.intermediate.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.8.attention.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.8.attention.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.4.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.9.intermediate.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.embeddings.word_embeddings.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.5.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.10.attention.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.1.attention.self.query.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.4.attention.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.6.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.9.attention.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.8.attention.self.key.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.5.intermediate.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.7.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.10.attention.self.value.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.1.output.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.0.attention.self.query.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.0.attention.self.query.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.7.intermediate.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.3.attention.self.query.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.9.output.LayerNorm.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.10.attention.self.key.bias', 'model.encoder._AutoEncoder__encoder.encoder.pooler.dense.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.9.attention.self.query.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.0.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.2.intermediate.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.5.attention.self.query.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.2.attention.self.query.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.2.attention.output.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.10.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.7.intermediate.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.7.output.LayerNorm.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.1.attention.self.key.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.0.attention.self.key.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.8.intermediate.dense.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.9.output.LayerNorm.weight', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.8.attention.self.value.bias', 'model.encoder._AutoEncoder__encoder.encoder.encoder.layer.0.intermediate.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /Users/jiayixian/cache/models--LightChen2333--joint-bert-slu-atis and are newly initialized: ['encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'pooler.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.bias', 'classifier.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'classifier.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#model = AutoModelForSequenceClassification.from_pretrained('LightChen2333/joint-bert-slu-atis', cache_dir=CACHE)\n",
    "config = AutoConfig.from_pretrained(model_path, cache_dir=CACHE)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, config=config, cache_dir=CACHE)\n",
    "# transformers.models.bert.tokenization_bert_fast.BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_id2label\": {\n",
       "    \"intent\": [\n",
       "      \"atis_flight\",\n",
       "      \"atis_airfare\",\n",
       "      \"atis_airline\",\n",
       "      \"atis_ground_service\",\n",
       "      \"atis_quantity\",\n",
       "      \"atis_city\",\n",
       "      \"atis_flight#atis_airfare\",\n",
       "      \"atis_abbreviation\",\n",
       "      \"atis_aircraft\",\n",
       "      \"atis_distance\",\n",
       "      \"atis_ground_fare\",\n",
       "      \"atis_capacity\",\n",
       "      \"atis_flight_time\",\n",
       "      \"atis_meal\",\n",
       "      \"atis_aircraft#atis_flight#atis_flight_no\",\n",
       "      \"atis_flight_no\",\n",
       "      \"atis_restriction\",\n",
       "      \"atis_airport\",\n",
       "      \"atis_airline#atis_flight_no\",\n",
       "      \"atis_cheapest\",\n",
       "      \"atis_ground_service#atis_ground_fare\"\n",
       "    ],\n",
       "    \"slot\": [\n",
       "      \"O\",\n",
       "      \"B-fromloc.city_name\",\n",
       "      \"B-toloc.city_name\",\n",
       "      \"B-round_trip\",\n",
       "      \"I-round_trip\",\n",
       "      \"B-cost_relative\",\n",
       "      \"B-fare_amount\",\n",
       "      \"I-fare_amount\",\n",
       "      \"B-arrive_date.month_name\",\n",
       "      \"B-arrive_date.day_number\",\n",
       "      \"I-fromloc.city_name\",\n",
       "      \"B-stoploc.city_name\",\n",
       "      \"B-arrive_time.time_relative\",\n",
       "      \"B-arrive_time.time\",\n",
       "      \"I-arrive_time.time\",\n",
       "      \"B-toloc.state_code\",\n",
       "      \"I-toloc.city_name\",\n",
       "      \"I-stoploc.city_name\",\n",
       "      \"B-meal_description\",\n",
       "      \"B-depart_date.month_name\",\n",
       "      \"B-depart_date.day_number\",\n",
       "      \"B-airline_name\",\n",
       "      \"I-airline_name\",\n",
       "      \"B-depart_time.period_of_day\",\n",
       "      \"B-depart_date.day_name\",\n",
       "      \"B-toloc.state_name\",\n",
       "      \"B-depart_time.time_relative\",\n",
       "      \"B-depart_time.time\",\n",
       "      \"B-toloc.airport_name\",\n",
       "      \"I-toloc.airport_name\",\n",
       "      \"B-depart_date.date_relative\",\n",
       "      \"B-or\",\n",
       "      \"B-airline_code\",\n",
       "      \"B-class_type\",\n",
       "      \"I-class_type\",\n",
       "      \"I-cost_relative\",\n",
       "      \"I-depart_time.time\",\n",
       "      \"B-fromloc.airport_name\",\n",
       "      \"I-fromloc.airport_name\",\n",
       "      \"B-city_name\",\n",
       "      \"B-flight_mod\",\n",
       "      \"B-meal\",\n",
       "      \"B-economy\",\n",
       "      \"B-fare_basis_code\",\n",
       "      \"I-depart_date.day_number\",\n",
       "      \"B-depart_date.today_relative\",\n",
       "      \"B-flight_stop\",\n",
       "      \"B-airport_code\",\n",
       "      \"B-fromloc.state_name\",\n",
       "      \"I-fromloc.state_name\",\n",
       "      \"I-city_name\",\n",
       "      \"B-connect\",\n",
       "      \"B-arrive_date.day_name\",\n",
       "      \"B-fromloc.state_code\",\n",
       "      \"B-arrive_date.today_relative\",\n",
       "      \"B-depart_date.year\",\n",
       "      \"B-depart_time.start_time\",\n",
       "      \"I-depart_time.start_time\",\n",
       "      \"B-depart_time.end_time\",\n",
       "      \"I-depart_time.end_time\",\n",
       "      \"B-arrive_time.start_time\",\n",
       "      \"B-arrive_time.end_time\",\n",
       "      \"I-arrive_time.end_time\",\n",
       "      \"I-flight_mod\",\n",
       "      \"B-flight_days\",\n",
       "      \"B-mod\",\n",
       "      \"B-flight_number\",\n",
       "      \"I-toloc.state_name\",\n",
       "      \"B-meal_code\",\n",
       "      \"I-meal_code\",\n",
       "      \"B-airport_name\",\n",
       "      \"I-airport_name\",\n",
       "      \"I-flight_stop\",\n",
       "      \"B-transport_type\",\n",
       "      \"I-transport_type\",\n",
       "      \"B-state_code\",\n",
       "      \"B-aircraft_code\",\n",
       "      \"B-toloc.country_name\",\n",
       "      \"I-arrive_date.day_number\",\n",
       "      \"B-toloc.airport_code\",\n",
       "      \"B-return_date.date_relative\",\n",
       "      \"I-return_date.date_relative\",\n",
       "      \"B-flight_time\",\n",
       "      \"I-economy\",\n",
       "      \"B-fromloc.airport_code\",\n",
       "      \"B-arrive_time.period_of_day\",\n",
       "      \"B-depart_time.period_mod\",\n",
       "      \"I-flight_time\",\n",
       "      \"B-return_date.day_name\",\n",
       "      \"B-arrive_date.date_relative\",\n",
       "      \"B-restriction_code\",\n",
       "      \"I-restriction_code\",\n",
       "      \"B-arrive_time.period_mod\",\n",
       "      \"I-arrive_time.period_of_day\",\n",
       "      \"B-period_of_day\",\n",
       "      \"B-stoploc.state_code\",\n",
       "      \"I-depart_date.today_relative\",\n",
       "      \"I-fare_basis_code\",\n",
       "      \"I-arrive_time.start_time\",\n",
       "      \"B-time\",\n",
       "      \"B-today_relative\",\n",
       "      \"I-today_relative\",\n",
       "      \"B-state_name\",\n",
       "      \"B-days_code\",\n",
       "      \"I-depart_time.period_of_day\",\n",
       "      \"I-arrive_time.time_relative\",\n",
       "      \"B-time_relative\",\n",
       "      \"I-time\",\n",
       "      \"B-return_date.month_name\",\n",
       "      \"B-return_date.day_number\",\n",
       "      \"I-depart_time.time_relative\",\n",
       "      \"B-stoploc.airport_name\",\n",
       "      \"B-day_name\",\n",
       "      \"B-month_name\",\n",
       "      \"B-day_number\",\n",
       "      \"B-return_time.period_mod\",\n",
       "      \"B-return_time.period_of_day\",\n",
       "      \"B-return_date.today_relative\",\n",
       "      \"I-return_date.today_relative\",\n",
       "      \"I-meal_description\"\n",
       "    ]\n",
       "  },\n",
       "  \"_label2id\": {\n",
       "    \"intent\": {\n",
       "      \"atis_abbreviation\": 7,\n",
       "      \"atis_aircraft\": 8,\n",
       "      \"atis_aircraft#atis_flight#atis_flight_no\": 14,\n",
       "      \"atis_airfare\": 1,\n",
       "      \"atis_airline\": 2,\n",
       "      \"atis_airline#atis_flight_no\": 18,\n",
       "      \"atis_airport\": 17,\n",
       "      \"atis_capacity\": 11,\n",
       "      \"atis_cheapest\": 19,\n",
       "      \"atis_city\": 5,\n",
       "      \"atis_distance\": 9,\n",
       "      \"atis_flight\": 0,\n",
       "      \"atis_flight#atis_airfare\": 6,\n",
       "      \"atis_flight_no\": 15,\n",
       "      \"atis_flight_time\": 12,\n",
       "      \"atis_ground_fare\": 10,\n",
       "      \"atis_ground_service\": 3,\n",
       "      \"atis_ground_service#atis_ground_fare\": 20,\n",
       "      \"atis_meal\": 13,\n",
       "      \"atis_quantity\": 4,\n",
       "      \"atis_restriction\": 16\n",
       "    },\n",
       "    \"slot\": {\n",
       "      \"B-aircraft_code\": 76,\n",
       "      \"B-airline_code\": 32,\n",
       "      \"B-airline_name\": 21,\n",
       "      \"B-airport_code\": 47,\n",
       "      \"B-airport_name\": 70,\n",
       "      \"B-arrive_date.date_relative\": 89,\n",
       "      \"B-arrive_date.day_name\": 52,\n",
       "      \"B-arrive_date.day_number\": 9,\n",
       "      \"B-arrive_date.month_name\": 8,\n",
       "      \"B-arrive_date.today_relative\": 54,\n",
       "      \"B-arrive_time.end_time\": 61,\n",
       "      \"B-arrive_time.period_mod\": 92,\n",
       "      \"B-arrive_time.period_of_day\": 85,\n",
       "      \"B-arrive_time.start_time\": 60,\n",
       "      \"B-arrive_time.time\": 13,\n",
       "      \"B-arrive_time.time_relative\": 12,\n",
       "      \"B-city_name\": 39,\n",
       "      \"B-class_type\": 33,\n",
       "      \"B-connect\": 51,\n",
       "      \"B-cost_relative\": 5,\n",
       "      \"B-day_name\": 112,\n",
       "      \"B-day_number\": 114,\n",
       "      \"B-days_code\": 103,\n",
       "      \"B-depart_date.date_relative\": 30,\n",
       "      \"B-depart_date.day_name\": 24,\n",
       "      \"B-depart_date.day_number\": 20,\n",
       "      \"B-depart_date.month_name\": 19,\n",
       "      \"B-depart_date.today_relative\": 45,\n",
       "      \"B-depart_date.year\": 55,\n",
       "      \"B-depart_time.end_time\": 58,\n",
       "      \"B-depart_time.period_mod\": 86,\n",
       "      \"B-depart_time.period_of_day\": 23,\n",
       "      \"B-depart_time.start_time\": 56,\n",
       "      \"B-depart_time.time\": 27,\n",
       "      \"B-depart_time.time_relative\": 26,\n",
       "      \"B-economy\": 42,\n",
       "      \"B-fare_amount\": 6,\n",
       "      \"B-fare_basis_code\": 43,\n",
       "      \"B-flight_days\": 64,\n",
       "      \"B-flight_mod\": 40,\n",
       "      \"B-flight_number\": 66,\n",
       "      \"B-flight_stop\": 46,\n",
       "      \"B-flight_time\": 82,\n",
       "      \"B-fromloc.airport_code\": 84,\n",
       "      \"B-fromloc.airport_name\": 37,\n",
       "      \"B-fromloc.city_name\": 1,\n",
       "      \"B-fromloc.state_code\": 53,\n",
       "      \"B-fromloc.state_name\": 48,\n",
       "      \"B-meal\": 41,\n",
       "      \"B-meal_code\": 68,\n",
       "      \"B-meal_description\": 18,\n",
       "      \"B-mod\": 65,\n",
       "      \"B-month_name\": 113,\n",
       "      \"B-or\": 31,\n",
       "      \"B-period_of_day\": 94,\n",
       "      \"B-restriction_code\": 90,\n",
       "      \"B-return_date.date_relative\": 80,\n",
       "      \"B-return_date.day_name\": 88,\n",
       "      \"B-return_date.day_number\": 109,\n",
       "      \"B-return_date.month_name\": 108,\n",
       "      \"B-return_date.today_relative\": 117,\n",
       "      \"B-return_time.period_mod\": 115,\n",
       "      \"B-return_time.period_of_day\": 116,\n",
       "      \"B-round_trip\": 3,\n",
       "      \"B-state_code\": 75,\n",
       "      \"B-state_name\": 102,\n",
       "      \"B-stoploc.airport_name\": 111,\n",
       "      \"B-stoploc.city_name\": 11,\n",
       "      \"B-stoploc.state_code\": 95,\n",
       "      \"B-time\": 99,\n",
       "      \"B-time_relative\": 106,\n",
       "      \"B-today_relative\": 100,\n",
       "      \"B-toloc.airport_code\": 79,\n",
       "      \"B-toloc.airport_name\": 28,\n",
       "      \"B-toloc.city_name\": 2,\n",
       "      \"B-toloc.country_name\": 77,\n",
       "      \"B-toloc.state_code\": 15,\n",
       "      \"B-toloc.state_name\": 25,\n",
       "      \"B-transport_type\": 73,\n",
       "      \"I-airline_name\": 22,\n",
       "      \"I-airport_name\": 71,\n",
       "      \"I-arrive_date.day_number\": 78,\n",
       "      \"I-arrive_time.end_time\": 62,\n",
       "      \"I-arrive_time.period_of_day\": 93,\n",
       "      \"I-arrive_time.start_time\": 98,\n",
       "      \"I-arrive_time.time\": 14,\n",
       "      \"I-arrive_time.time_relative\": 105,\n",
       "      \"I-city_name\": 50,\n",
       "      \"I-class_type\": 34,\n",
       "      \"I-cost_relative\": 35,\n",
       "      \"I-depart_date.day_number\": 44,\n",
       "      \"I-depart_date.today_relative\": 96,\n",
       "      \"I-depart_time.end_time\": 59,\n",
       "      \"I-depart_time.period_of_day\": 104,\n",
       "      \"I-depart_time.start_time\": 57,\n",
       "      \"I-depart_time.time\": 36,\n",
       "      \"I-depart_time.time_relative\": 110,\n",
       "      \"I-economy\": 83,\n",
       "      \"I-fare_amount\": 7,\n",
       "      \"I-fare_basis_code\": 97,\n",
       "      \"I-flight_mod\": 63,\n",
       "      \"I-flight_stop\": 72,\n",
       "      \"I-flight_time\": 87,\n",
       "      \"I-fromloc.airport_name\": 38,\n",
       "      \"I-fromloc.city_name\": 10,\n",
       "      \"I-fromloc.state_name\": 49,\n",
       "      \"I-meal_code\": 69,\n",
       "      \"I-meal_description\": 119,\n",
       "      \"I-restriction_code\": 91,\n",
       "      \"I-return_date.date_relative\": 81,\n",
       "      \"I-return_date.today_relative\": 118,\n",
       "      \"I-round_trip\": 4,\n",
       "      \"I-stoploc.city_name\": 17,\n",
       "      \"I-time\": 107,\n",
       "      \"I-today_relative\": 101,\n",
       "      \"I-toloc.airport_name\": 29,\n",
       "      \"I-toloc.city_name\": 16,\n",
       "      \"I-toloc.state_name\": 67,\n",
       "      \"I-transport_type\": 74,\n",
       "      \"O\": 0\n",
       "    }\n",
       "  },\n",
       "  \"_name_or_path\": \"/Users/jiayixian/cache/models--LightChen2333--joint-bert-slu-atis\",\n",
       "  \"_num_labels\": {\n",
       "    \"intent\": 21,\n",
       "    \"slot\": 120\n",
       "  },\n",
       "  \"architectures\": [\n",
       "    \"PretrainedModelForSLUToSave\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"is_decoder\": true,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model\": {\n",
       "    \"_model_target_\": \"model.open_slu_model.OpenSLUModel\",\n",
       "    \"decoder\": {\n",
       "      \"_model_target_\": \"model.decoder.base_decoder.BaseDecoder\",\n",
       "      \"intent_classifier\": {\n",
       "        \"_model_target_\": \"model.decoder.classifier.LinearClassifier\",\n",
       "        \"ignore_index\": -100,\n",
       "        \"input_dim\": 768,\n",
       "        \"intent_label_num\": 21,\n",
       "        \"mode\": \"intent\",\n",
       "        \"use_intent\": true\n",
       "      },\n",
       "      \"slot_classifier\": {\n",
       "        \"_model_target_\": \"model.decoder.classifier.LinearClassifier\",\n",
       "        \"ignore_index\": -100,\n",
       "        \"input_dim\": 768,\n",
       "        \"mode\": \"slot\",\n",
       "        \"slot_label_num\": 120,\n",
       "        \"use_slot\": true\n",
       "      }\n",
       "    },\n",
       "    \"encoder\": {\n",
       "      \"_model_target_\": \"model.encoder.AutoEncoder\",\n",
       "      \"encoder_name\": \"bert-base-uncased\",\n",
       "      \"output_dim\": 768,\n",
       "      \"return_sentence_level_hidden\": true,\n",
       "      \"return_with_input\": true\n",
       "    },\n",
       "    \"ignore_index\": -100\n",
       "  },\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"return_dict\": false,\n",
       "  \"tokenizer\": {\n",
       "    \"_align_mode_\": \"general\",\n",
       "    \"_padding_side_\": \"right\",\n",
       "    \"_tokenizer_name_\": \"bert-base-uncased\",\n",
       "    \"add_special_tokens\": true\n",
       "  },\n",
       "  \"tokenizer_class\": \"OpenSLUv1\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.26.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label2id = {v: i for i, v in enumerate(config._label2id[\"intent\"])}\n",
    "id2label = {i: v for v, i in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "config = AutoConfig.from_pretrained(model_path,\n",
    "                                    num_labels=num_labels,\n",
    "                                    label2id=label2id,\n",
    "                                    id2label=id2label,\n",
    "                                    cache_dir=CACHE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config._label2id[\"intent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[1045, 2052, 2066, 2000, 2424, 1037, 3462, 2013, 5904, 2000, 5869, 7136,\n",
      "         2008, 3084, 1037, 2644, 1999, 2358, 1012, 3434]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 21 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jiayixian/projects/vis_nlp/playground.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiayixian/projects/vis_nlp/playground.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInput IDs:\u001b[39m\u001b[39m\"\u001b[39m, input_ids)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiayixian/projects/vis_nlp/playground.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([num_labels])\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jiayixian/projects/vis_nlp/playground.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39;49minput_ids, labels\u001b[39m=\u001b[39;49mlabels, return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiayixian/projects/vis_nlp/playground.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLogits:\u001b[39m\u001b[39m\"\u001b[39m, outputs\u001b[39m.\u001b[39mlogits)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py39/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1598\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1596\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mproblem_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msingle_label_classification\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1597\u001b[0m     loss_fct \u001b[39m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m-> 1598\u001b[0m     loss \u001b[39m=\u001b[39m loss_fct(logits\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_labels), labels\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m   1599\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mproblem_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulti_label_classification\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1600\u001b[0m     loss_fct \u001b[39m=\u001b[39m BCEWithLogitsLoss()\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py39/lib/python3.9/site-packages/torch/nn/modules/loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1164\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1165\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1166\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py39/lib/python3.9/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 21 is out of bounds."
     ]
    }
   ],
   "source": [
    "with open(os.path.join(model_path, 'tokenizer.pkl'), 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "tokens = tokenizer.tokenize(\"i would like to find a flight from charlotte to las vegas that makes a stop in st. louis\")\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "labels = torch.tensor([num_labels]).unsqueeze(0)\n",
    "outputs = model(input_ids=input_ids, labels=labels, return_dict=True)\n",
    "print(\"Logits:\", outputs.logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3241, -0.2763]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.modeling_bert.BertForSequenceClassification"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 13 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jiayixian/projects/vis_nlp/playground.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jiayixian/projects/vis_nlp/playground.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(\u001b[39m\"\u001b[39m\u001b[39mHello, my dog is cute\u001b[39m\u001b[39m\"\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jiayixian/projects/vis_nlp/playground.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m13\u001b[39m])\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)  \u001b[39m# Batch size 1\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jiayixian/projects/vis_nlp/playground.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, labels\u001b[39m=\u001b[39;49mlabels)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py39/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1598\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1596\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mproblem_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msingle_label_classification\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1597\u001b[0m     loss_fct \u001b[39m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m-> 1598\u001b[0m     loss \u001b[39m=\u001b[39m loss_fct(logits\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_labels), labels\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m   1599\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mproblem_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulti_label_classification\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1600\u001b[0m     loss_fct \u001b[39m=\u001b[39m BCEWithLogitsLoss()\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py39/lib/python3.9/site-packages/torch/nn/modules/loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1164\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1165\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1166\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py39/lib/python3.9/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 13 is out of bounds."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([13]).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.8278, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1355, -0.1178]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tokenizer class OpenSLUv1 does not exist or is not currently imported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jiayixian/projects/vis_nlp/playground.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiayixian/projects/vis_nlp/playground.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(model_path, cache_dir\u001b[39m=\u001b[39mCACHE)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiayixian/projects/vis_nlp/playground.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# hidden_dropout_prob = args.hidden_dropout_prob,\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiayixian/projects/vis_nlp/playground.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# attention_probs_dropout_prob = model_args.attention_probs_dropout_prob\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiayixian/projects/vis_nlp/playground.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jiayixian/projects/vis_nlp/playground.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#model = BertForSequenceClassification(config, no_cuda=True)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jiayixian/projects/vis_nlp/playground.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(model_path, config\u001b[39m=\u001b[39;49mconfig, cache_dir \u001b[39m=\u001b[39;49m CACHE, use_fast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py39/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:655\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m         tokenizer_class \u001b[39m=\u001b[39m tokenizer_class_from_name(tokenizer_class_candidate)\n\u001b[1;32m    654\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    656\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTokenizer class \u001b[39m\u001b[39m{\u001b[39;00mtokenizer_class_candidate\u001b[39m}\u001b[39;00m\u001b[39m does not exist or is not currently imported.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    657\u001b[0m         )\n\u001b[1;32m    658\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    660\u001b[0m \u001b[39m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[39m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Tokenizer class OpenSLUv1 does not exist or is not currently imported."
     ]
    }
   ],
   "source": [
    "\"\"\"config = AutoConfig.from_pretrained(path=model_path,\n",
    "                                    num_labels=num_labels,\n",
    "                                    labels2id=label2id,\n",
    "                                    id2label=id2label,\n",
    "                                    cache_dir=CACHE)\"\"\"\n",
    "config = AutoConfig.from_pretrained(model_path, cache_dir=CACHE)\n",
    "# hidden_dropout_prob = args.hidden_dropout_prob,\n",
    "# attention_probs_dropout_prob = model_args.attention_probs_dropout_prob\n",
    "\n",
    "#model = BertForSequenceClassification(config, no_cuda=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, config=config, cache_dir = CACHE, use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, config=config, cache_dir = CACHE, use_fast=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME\n",
    "\n",
    "`exp = explainer.explain_instance(a string of text, model.predict: output 1d ndarray of probs , num_features=6)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = LimeTextExplainer(class_names=class_names) # class_names: a list of strings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84faba2c3d73f72195a31da7dbd07f0b601f44c80958afd4e19c4d66e3eef367"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
